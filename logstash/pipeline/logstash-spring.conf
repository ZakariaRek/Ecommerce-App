input {
  kafka {
    bootstrap_servers => "kafka:29092"
    topics => ["app-logs"]
    codec => "json"
    group_id => "logstash-microservices-group"
    client_id => "logstash-consumer"
    auto_offset_reset => "earliest"
    consumer_threads => 1
  }
}

filter {
  # Only process if JSON parsing succeeded
  if "_jsonparsefailure" not in [tags] {

    # Add processing timestamp
    mutate {
      add_field => { "logstash_processed_at" => "%{+yyyy-MM-dd HH:mm:ss}" }
    }

    # Fix timestamp parsing - handle multiple formats
    if [@timestamp] {
      date {
        match => [
          "@timestamp",
          "ISO8601",
          "yyyy-MM-dd'T'HH:mm:ss.SSSZ",
          "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'",
          "yyyy-MM-dd'T'HH:mm:ss'Z'"
        ]
        target => "@timestamp"
      }
    }

    # Ensure required fields exist
    if ![level] {
      mutate { add_field => { "level" => "INFO" } }
    }
    if ![service_name] {
      mutate { add_field => { "service_name" => "unknown" } }
    }
    if ![logger_name] {
      mutate { add_field => { "logger_name" => "unknown" } }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "microservices-logs-%{+yyyy.MM.dd}"
    document_type => "_doc"
  }

  # Remove in production
  stdout {
    codec => rubydebug
  }
}